{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kesperinc/LLaMA_usage_example/blob/master/Document_Classification_Using_KR_SBERT_via_Transformers_(new%2C_including_data_preprocessing%2C_last_update_2022_05_03).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_YTfDPSga_F"
      },
      "source": [
        "# Document Classification Using KR-SBERT via Transformers\n",
        "\n",
        "*SNU NLP Laboratory*\n",
        "\n",
        "In this tutorial, we will show how to apply our pre-trained KoRean S-BERT model to a document classification task, using HuggingFace's `transformers` library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv5o7uZmcQAB"
      },
      "source": [
        "## 0. Preparation\n",
        "\n",
        "## Libraries\n",
        "\n",
        "First, you need to install the following libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZFeNxok95F8"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers sentence-transformers kss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udlz6OUqcYYy"
      },
      "source": [
        "### BNC dataset\n",
        "\n",
        "Then the Balanced News Corpus for a sentiment classification task.\n",
        "\n",
        "Download and unzip this file.\n",
        "\n",
        "üëáüëáüëáüëáüëá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAhBKWOlpBYJ"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Lg2jL89n3lqkKCulAnk4WwmI8G1hNfIA' -O BalancedNewsCorpusShuffled.zip\n",
        "!unzip BalancedNewsCorpusShuffled.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxOq4BMKeetn"
      },
      "source": [
        "## 1. Setting on Python\n",
        "\n",
        "Now we can import all of the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNpNL8SQ836P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For Transformer models\n",
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# For train/dev/test datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import random_split\n",
        "from torch.nn.functional import pad\n",
        "\n",
        "# For evaluation\n",
        "from torch import manual_seed\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntyRXOlFvz6Y"
      },
      "source": [
        "Let us load a `SentenceTransformer` model for sentence embddings and a `BertForSequenceClassification` for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMCnjMGJ--in"
      },
      "outputs": [],
      "source": [
        "sbert_model_name = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'\n",
        "sbert_model = SentenceTransformer(sbert_model_name)\n",
        "# config = sbert_model._first_module().auto_model.config # for bert token embeddings\n",
        "from transformers import BertConfig\n",
        "config = BertConfig()\n",
        "config.num_labels=9\n",
        "config.max_position_embeddings = sbert_model.max_seq_length\n",
        "model = BertForSequenceClassification(config)\n",
        "model.main_input_name = 'inputs_embeds'\n",
        "max_seq_length = sbert_model.max_seq_length\n",
        "manual_seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVuNnOulqJ_3"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p7q5_l5g9Yl"
      },
      "source": [
        "## 2. Building the BNC datasets\n",
        "\n",
        "We define a new `Dataset` class loading the Balanced News Corpus dataset for the `BertForSequenceClassification`model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGLhUBxER1Yr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean(text:str):\n",
        "  # https://github.com/YongWookHa/kor-text-preprocess/blob/master/src/clean.py\n",
        "  not_used = re.compile('[^ .?!/@$%~|0-9|„Ñ±-„Ö£Í∞Ä-Ìû£]+')\n",
        "  dup_space = re.compile('[ \\t]+')  # white space duplicate\n",
        "  dup_stop = re.compile('[\\.]+')  # full stop duplicate\n",
        "\n",
        "  cleaned = not_used.sub('', text.strip())\n",
        "  cleaned = dup_space.sub(' ', cleaned)\n",
        "  cleaned = dup_stop.sub('.', cleaned)\n",
        "\n",
        "  return cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE4QFtPgMft6"
      },
      "outputs": [],
      "source": [
        "# from kss import split_sentences # Sentence segmentation for the Korean Language\n",
        "# sent_tokenize = split_sentences\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSEFSrxBEf1l"
      },
      "outputs": [],
      "source": [
        "def get_sentence_embeddings(text:str, cls_token='[CLS]', sep_token='[SEP]', padding=True, truncate=True, max_len=128):\n",
        "  sentences = [cls_token] + sent_tokenize(text) + [sep_token]\n",
        "  embeddings = sbert_model.encode(sentences, convert_to_tensor=True)\n",
        "  d = sbert_model.get_sentence_embedding_dimension()\n",
        "  n = len(sentences)\n",
        "\n",
        "  seq_len = n\n",
        "\n",
        "  if padding:\n",
        "    seq_len = max(n, max_len)\n",
        "\n",
        "  if truncate:\n",
        "    seq_len = min(seq_len, max_len)\n",
        "\n",
        "  output = torch.zeros((seq_len, d), dtype=torch.float32).to(device)\n",
        "  for i in range(min(n, seq_len)):\n",
        "    output[i] = embeddings[i]\n",
        "\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHQHhX-l_e0Q"
      },
      "outputs": [],
      "source": [
        "class BNCDataset(Dataset):\n",
        "\n",
        "    labels = ['IT/Í≥ºÌïô', 'Í≤ΩÏ†ú', 'Î¨∏Ìôî', 'ÎØ∏Ïö©/Í±¥Í∞ï', 'ÏÇ¨Ìöå', 'ÏÉùÌôú', 'Ïä§Ìè¨Ï∏†', 'Ïó∞Ïòà', 'Ï†ïÏπò']\n",
        "\n",
        "    def __init__(self, data_file='BalancedNewsCorpus_train.csv'):\n",
        "        data = pd.read_csv(data_file)\n",
        "        self.text = data['News'].apply(lambda text: text.replace('<p>', '\\n').replace('</p>', '\\n'))\n",
        "        self.text = self.text.apply(clean).tolist()\n",
        "        self.label = data['Topic'].apply(lambda label: self.labels.index(label)).tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text[idx]\n",
        "        label = torch.tensor(self.label[idx]).to(device)\n",
        "        feature = {'inputs_embeds': get_sentence_embeddings(text), 'labels': label}\n",
        "        return feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNciffUPwtpL"
      },
      "source": [
        "Load the BNC dataset files we have downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAzDMOCHAPJr"
      },
      "outputs": [],
      "source": [
        "train_dataset = BNCDataset('BalancedNewsCorpus_train.csv')\n",
        "test_dataset = BNCDataset('BalancedNewsCorpus_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u41_9tigLBbs"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset = random_split(train_dataset, [8100, 900], generator=manual_seed(1234))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ZPMCuow8O6"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bYRwah9tAitP"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"./bnc-results\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    # eval_steps=10,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    dataloader_pin_memory=True, # False for GPU\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzMERJh-xF0Z"
      },
      "source": [
        "We will evaluate our classifier using Accuracy, F1, Precision, and Recall scores. This should be defined as the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ns82ofw7_Lp0"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    print(confusion_matrix(labels, preds))\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s-xEQJexTjp"
      },
      "source": [
        "Instantiate the `Trainer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qTgZbZYMAmCU"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKGL6ICOxBN9"
      },
      "source": [
        "Let's train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG-6QEQxAo19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749746ba-81d0-4b78-b6f0-4921636458ea"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 8100\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 64\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWwOb9Gow-rL"
      },
      "source": [
        "## 4. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn32rrItAp4a"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(test_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}